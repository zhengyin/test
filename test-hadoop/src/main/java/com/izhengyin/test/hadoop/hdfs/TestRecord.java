/**
 * Autogenerated by Avro
 * 
 * DO NOT EDIT DIRECTLY
 */
package com.izhengyin.test.hadoop.hdfs;  
@SuppressWarnings("all")
/** Test records */
@org.apache.avro.specific.AvroGenerated
public class TestRecord extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"TestRecord\",\"namespace\":\"com.izhengyin.test.hadoop.hdfs\",\"doc\":\"Test records\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"},{\"name\":\"_partition\",\"type\":\"int\"},{\"name\":\"_offset\",\"type\":\"long\"}]}");
  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
  @Deprecated public int id;
  @Deprecated public CharSequence name;
  @Deprecated public int age;
  @Deprecated public int _partition;
  @Deprecated public long _offset;

  /**
   * Default constructor.  Note that this does not initialize fields
   * to their default values from the schema.  If that is desired then
   * one should use <code>newBuilder()</code>. 
   */
  public TestRecord() {}

  /**
   * All-args constructor.
   */
  public TestRecord(Integer id, CharSequence name, Integer age, Integer _partition, Long _offset) {
    this.id = id;
    this.name = name;
    this.age = age;
    this._partition = _partition;
    this._offset = _offset;
  }

  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public Object get(int field$) {
    switch (field$) {
    case 0: return id;
    case 1: return name;
    case 2: return age;
    case 3: return _partition;
    case 4: return _offset;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
  // Used by DatumReader.  Applications should not call. 
  @SuppressWarnings(value="unchecked")
  public void put(int field$, Object value$) {
    switch (field$) {
    case 0: id = (Integer)value$; break;
    case 1: name = (CharSequence)value$; break;
    case 2: age = (Integer)value$; break;
    case 3: _partition = (Integer)value$; break;
    case 4: _offset = (Long)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'id' field.
   */
  public Integer getId() {
    return id;
  }

  /**
   * Sets the value of the 'id' field.
   * @param value the value to set.
   */
  public void setId(Integer value) {
    this.id = value;
  }

  /**
   * Gets the value of the 'name' field.
   */
  public CharSequence getName() {
    return name;
  }

  /**
   * Sets the value of the 'name' field.
   * @param value the value to set.
   */
  public void setName(CharSequence value) {
    this.name = value;
  }

  /**
   * Gets the value of the 'age' field.
   */
  public Integer getAge() {
    return age;
  }

  /**
   * Sets the value of the 'age' field.
   * @param value the value to set.
   */
  public void setAge(Integer value) {
    this.age = value;
  }

  /**
   * Gets the value of the '_partition' field.
   */
  public Integer getPartition$1() {
    return _partition;
  }

  /**
   * Sets the value of the '_partition' field.
   * @param value the value to set.
   */
  public void setPartition$1(Integer value) {
    this._partition = value;
  }

  /**
   * Gets the value of the '_offset' field.
   */
  public Long getOffset$1() {
    return _offset;
  }

  /**
   * Sets the value of the '_offset' field.
   * @param value the value to set.
   */
  public void setOffset$1(Long value) {
    this._offset = value;
  }

  /** Creates a new TestRecord RecordBuilder */
  public static com.izhengyin.test.hadoop.hdfs.TestRecord.Builder newBuilder() {
    return new com.izhengyin.test.hadoop.hdfs.TestRecord.Builder();
  }
  
  /** Creates a new TestRecord RecordBuilder by copying an existing Builder */
  public static com.izhengyin.test.hadoop.hdfs.TestRecord.Builder newBuilder(com.izhengyin.test.hadoop.hdfs.TestRecord.Builder other) {
    return new com.izhengyin.test.hadoop.hdfs.TestRecord.Builder(other);
  }
  
  /** Creates a new TestRecord RecordBuilder by copying an existing TestRecord instance */
  public static com.izhengyin.test.hadoop.hdfs.TestRecord.Builder newBuilder(com.izhengyin.test.hadoop.hdfs.TestRecord other) {
    return new com.izhengyin.test.hadoop.hdfs.TestRecord.Builder(other);
  }
  
  /**
   * RecordBuilder for TestRecord instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<TestRecord>
    implements org.apache.avro.data.RecordBuilder<TestRecord> {

    private int id;
    private CharSequence name;
    private int age;
    private int _partition;
    private long _offset;

    /** Creates a new Builder */
    private Builder() {
      super(com.izhengyin.test.hadoop.hdfs.TestRecord.SCHEMA$);
    }
    
    /** Creates a Builder by copying an existing Builder */
    private Builder(com.izhengyin.test.hadoop.hdfs.TestRecord.Builder other) {
      super(other);
      if (isValidValue(fields()[0], other.id)) {
        this.id = data().deepCopy(fields()[0].schema(), other.id);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.name)) {
        this.name = data().deepCopy(fields()[1].schema(), other.name);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.age)) {
        this.age = data().deepCopy(fields()[2].schema(), other.age);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other._partition)) {
        this._partition = data().deepCopy(fields()[3].schema(), other._partition);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other._offset)) {
        this._offset = data().deepCopy(fields()[4].schema(), other._offset);
        fieldSetFlags()[4] = true;
      }
    }
    
    /** Creates a Builder by copying an existing TestRecord instance */
    private Builder(com.izhengyin.test.hadoop.hdfs.TestRecord other) {
            super(com.izhengyin.test.hadoop.hdfs.TestRecord.SCHEMA$);
      if (isValidValue(fields()[0], other.id)) {
        this.id = data().deepCopy(fields()[0].schema(), other.id);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.name)) {
        this.name = data().deepCopy(fields()[1].schema(), other.name);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.age)) {
        this.age = data().deepCopy(fields()[2].schema(), other.age);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other._partition)) {
        this._partition = data().deepCopy(fields()[3].schema(), other._partition);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other._offset)) {
        this._offset = data().deepCopy(fields()[4].schema(), other._offset);
        fieldSetFlags()[4] = true;
      }
    }

    /** Gets the value of the 'id' field */
    public Integer getId() {
      return id;
    }
    
    /** Sets the value of the 'id' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder setId(int value) {
      validate(fields()[0], value);
      this.id = value;
      fieldSetFlags()[0] = true;
      return this; 
    }
    
    /** Checks whether the 'id' field has been set */
    public boolean hasId() {
      return fieldSetFlags()[0];
    }
    
    /** Clears the value of the 'id' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder clearId() {
      fieldSetFlags()[0] = false;
      return this;
    }

    /** Gets the value of the 'name' field */
    public CharSequence getName() {
      return name;
    }
    
    /** Sets the value of the 'name' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder setName(CharSequence value) {
      validate(fields()[1], value);
      this.name = value;
      fieldSetFlags()[1] = true;
      return this; 
    }
    
    /** Checks whether the 'name' field has been set */
    public boolean hasName() {
      return fieldSetFlags()[1];
    }
    
    /** Clears the value of the 'name' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder clearName() {
      name = null;
      fieldSetFlags()[1] = false;
      return this;
    }

    /** Gets the value of the 'age' field */
    public Integer getAge() {
      return age;
    }
    
    /** Sets the value of the 'age' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder setAge(int value) {
      validate(fields()[2], value);
      this.age = value;
      fieldSetFlags()[2] = true;
      return this; 
    }
    
    /** Checks whether the 'age' field has been set */
    public boolean hasAge() {
      return fieldSetFlags()[2];
    }
    
    /** Clears the value of the 'age' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder clearAge() {
      fieldSetFlags()[2] = false;
      return this;
    }

    /** Gets the value of the '_partition' field */
    public Integer getPartition$1() {
      return _partition;
    }
    
    /** Sets the value of the '_partition' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder setPartition$1(int value) {
      validate(fields()[3], value);
      this._partition = value;
      fieldSetFlags()[3] = true;
      return this; 
    }
    
    /** Checks whether the '_partition' field has been set */
    public boolean hasPartition$1() {
      return fieldSetFlags()[3];
    }
    
    /** Clears the value of the '_partition' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder clearPartition$1() {
      fieldSetFlags()[3] = false;
      return this;
    }

    /** Gets the value of the '_offset' field */
    public Long getOffset$1() {
      return _offset;
    }
    
    /** Sets the value of the '_offset' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder setOffset$1(long value) {
      validate(fields()[4], value);
      this._offset = value;
      fieldSetFlags()[4] = true;
      return this; 
    }
    
    /** Checks whether the '_offset' field has been set */
    public boolean hasOffset$1() {
      return fieldSetFlags()[4];
    }
    
    /** Clears the value of the '_offset' field */
    public com.izhengyin.test.hadoop.hdfs.TestRecord.Builder clearOffset$1() {
      fieldSetFlags()[4] = false;
      return this;
    }

    @Override
    public TestRecord build() {
      try {
        TestRecord record = new TestRecord();
        record.id = fieldSetFlags()[0] ? this.id : (Integer) defaultValue(fields()[0]);
        record.name = fieldSetFlags()[1] ? this.name : (CharSequence) defaultValue(fields()[1]);
        record.age = fieldSetFlags()[2] ? this.age : (Integer) defaultValue(fields()[2]);
        record._partition = fieldSetFlags()[3] ? this._partition : (Integer) defaultValue(fields()[3]);
        record._offset = fieldSetFlags()[4] ? this._offset : (Long) defaultValue(fields()[4]);
        return record;
      } catch (Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }
}
